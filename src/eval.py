import ee
import tensorflow as tf
import os
import numpy as np

from train import addNDVI, parse_tfrecord, toTuple

outputBucket = "/home/burn/Downloads/ee-docs-demos-berlin"

# Names for output files.
testFilePrefix = 'Testing_demo_'

fileNameSuffix = 'ee_export.tfrecord.gz'

testFilePath = outputBucket + '/' + testFilePrefix + fileNameSuffix

model = tf.keras.models.load_model("./model_path")
model.compile(optimizer=tf.train.AdamOptimizer(),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# TODO: Move this to beam pipeline source
testDataset = (
    tf.data.TFRecordDataset(testFilePath, compression_type='GZIP')
        .map(parse_tfrecord, num_parallel_calls=5)
        .map(addNDVI)
        .map(toTuple)
        .batch(1)
)

model.evaluate(testDataset, steps=100)

# Use the trained model

# Get a list of all the files in the output bucket.
filesList = os.listdir(outputBucket)


# Get only the files generated by the image export.
imageFilePrefix = 'Image_pixel_demo_'
exportFilesList = [s for s in filesList if imageFilePrefix in s]

# Get the list of image files and the JSON mixer file.
imageFilesList = []
jsonFile = None
for f in exportFilesList:
    if f.endswith('.tfrecord.gz'):
        imageFilesList.append(outputBucket + "/" + f)
    elif f.endswith('.json'):
        jsonFile = outputBucket + "/" + f

# Make sure the files are in the right order.
imageFilesList.sort()

print("######################################")
print(imageFilesList)
print(jsonFile)



import json

# Load the contents of the mixer file to a JSON object.
with open(jsonFile) as jsonFile:
    mixer = json.load(jsonFile)

print(mixer)


# Get relevant info from the JSON mixer file.
PATCH_WIDTH = mixer['patchDimensions'][0]
PATCH_HEIGHT = mixer['patchDimensions'][1]
PATCHES = mixer['totalPatches']
PATCH_DIMENSIONS_FLAT = [PATCH_WIDTH * PATCH_HEIGHT, 1]

bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7']

# Note that the tensors are in the shape of a patch, one patch for each band.
imageColumns = [
    tf.FixedLenFeature(shape=PATCH_DIMENSIONS_FLAT, dtype=tf.float32)
    for k in bands
]

# Parsing dictionary.
imageFeaturesDict = dict(zip(bands, imageColumns))

# Note that you can make one dataset from many files by specifying a list.
imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')

# Parsing function.
def parse_image(example_proto):
    return tf.parse_single_example(example_proto, imageFeaturesDict)

# Parse the data into tensors, one long tensor per patch.
imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)

# Break our long tensors into many little ones.
imageDataset = imageDataset.flat_map(
    lambda features: tf.data.Dataset.from_tensor_slices(features)
)

# Add additional features (NDVI).
imageDataset = imageDataset.map(
    # Add NDVI to a feature that doesn't have a label.
    lambda features: addNDVI(features, None)[0]
)

# Turn the dictionary in each record into a tuple with a dummy label.
imageDataset = imageDataset.map(
    # Add a dummy target (-1), with a value that is obviously ridiculous.
    # This is because the model expects a tuple of (inputs, label).
    lambda dataDict: (tf.transpose(list(dataDict.values())), tf.constant(-1))
)

# Turn each patch into a batch.
imageDataset = imageDataset.batch(PATCH_WIDTH * PATCH_HEIGHT)


# Run prediction in batches, with as many steps as there are patches.
predictions = model.predict(imageDataset, steps=PATCHES, verbose=1)

# Note that the predictions come as a numpy array.  Check the first one.
print(predictions[0])


outputImageFile =  outputBucket + '/Classified_pixel_demo.TFRecord'
print('Writing to file ' + outputImageFile)

# Instantiate the writer.
writer = tf.python_io.TFRecordWriter(outputImageFile)

# Every patch-worth of predictions we'll dump an example into the output
# file with a single feature that holds our predictions. Since our predictions
# are already in the order of the exported data, the patches we create here
# will also be in the right order.
patch = [[], [], [], []]
curPatch = 1
print(len(predictions))
for prediction in predictions:
    patch[0].append(np.argmax(prediction, 1)[0])
    patch[1].append(prediction[0][0])
    patch[2].append(prediction[0][1])
    patch[3].append(prediction[0][2])
    # Once we've seen a patches-worth of class_ids...
    if (len(patch[0]) == PATCH_WIDTH * PATCH_HEIGHT):
        print('Done with patch ' + str(curPatch) + ' of ' + str(PATCHES) + '...')
        # Create an example
        example = tf.train.Example(
            features=tf.train.Features(
                feature={
                    'prediction': tf.train.Feature(
                        int64_list=tf.train.Int64List(
                            value=patch[0])),
                    'bareProb': tf.train.Feature(
                        float_list=tf.train.FloatList(
                            value=patch[1])),
                    'vegProb': tf.train.Feature(
                        float_list=tf.train.FloatList(
                            value=patch[2])),
                    'waterProb': tf.train.Feature(
                        float_list=tf.train.FloatList(
                            value=patch[3])),
                }
            )
        )
        # Write the example to the file and clear our patch array so it's ready for
        # another batch of class ids
        print(example.SerializeToString())
        writer.write(example.SerializeToString())
        patch = [[], [], [], []]
        curPatch += 1

writer.close()
