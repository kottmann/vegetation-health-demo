
from __future__ import absolute_import
import os
import argparse
import logging
import glob
import tensorflow as tf
import apache_beam as beam
from apache_beam.io import tfrecordio
from apache_beam.io.filesystem import CompressionTypes
from apache_beam.metrics import Metrics
from apache_beam.metrics.metric import MetricsFilter
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.options.pipeline_options import SetupOptions
from apache_beam.options.pipeline_options import StandardOptions
import json


def run(argv=None):
    parser = argparse.ArgumentParser()
    #parser.add_argument('--input',
    #                    dest='input',
    #                    help='Input folder to process.')
    #parser.add_argument('--output',
    #                    dest='output',
    #                    required=True,
    #                    help='Output folder to write results to.')
    #parser.add_argument('--models',
    #                    dest='models',
    #                    help='Input folder to read model parameters.')
    #parser.add_argument('--batchsize',
    #                    dest='batchsize',
    #                    help='Batch size for processing')
    known_args, pipeline_args = parser.parse_known_args(argv)

    # We use the save_main_session option because one or more DoFn's in this
    # workflow rely on global context (e.g., a module imported at module level).
    pipeline_options = PipelineOptions(pipeline_args)
    pipeline_options.view_as(SetupOptions).save_main_session = True
    pipeline_options.view_as(StandardOptions).streaming = True

    outputBucket = "/home/burn/Downloads/ee-docs-demos-berlin"

    # Names for output files.
    testFilePrefix = 'Testing_demo_'

    fileNameSuffix = 'ee_export.tfrecord.gz'

    # Get a list of all the files in the output bucket.
    filesList = os.listdir(outputBucket)


    # Get only the files generated by the image export.
    imageFilePrefix = 'Image_pixel_demo_'
    exportFilesList = [s for s in filesList if imageFilePrefix in s]

    # Get the list of image files and the JSON mixer file.
    imageFilesList = []
    jsonFile = None
    for f in exportFilesList:
        if f.endswith('.tfrecord.gz'):
            imageFilesList.append(outputBucket + "/" + f)
        elif f.endswith('.json'):
            jsonFile = outputBucket + "/" + f

    # Get relevant info from the JSON mixer file.
    with open(jsonFile) as jsonFile:
        mixer = json.load(jsonFile)

    PATCH_WIDTH = mixer['patchDimensions'][0]
    PATCH_HEIGHT = mixer['patchDimensions'][1]
    PATCHES = mixer['totalPatches']
    PATCH_DIMENSIONS_FLAT = [PATCH_WIDTH * PATCH_HEIGHT, 1]

    bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7']

    label = 'landcover'

    # This is list of all the properties we want to export.
    featureNames = list(bands)
    featureNames.append(label)

    # Note that the tensors are in the shape of a patch, one patch for each band.
    imageColumns = [
        tf.FixedLenFeature(shape=PATCH_DIMENSIONS_FLAT, dtype=tf.float32)
        for k in bands
    ]

    # Parsing dictionary.
    imageFeaturesDict = dict(zip(bands, imageColumns))

    # List of fixed-length features, all of which are float32.
    columns = [
        tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for k in featureNames
    ]

    # Dictionary with names as keys, features as values.
    featuresDict = dict(zip(featureNames, columns))

    # Parsing function.
    def parse_image(example_proto):
        return tf.parse_single_example(example_proto, imageFeaturesDict)

    def parse_tfrecord(example_proto):
        """The parsing function.

        Read a serialized example into the structure defined by featuresDict.

        Args:
          example_proto: a serialized Example.

        Returns:
          A tuple of the predictors dictionary and the label, cast to an `int32`.
        """
        parsed_features = tf.io.parse_single_example(example_proto, featuresDict)


        labels = parsed_features.pop(label)
        return parsed_features, tf.cast(labels, tf.int32)

    with beam.Pipeline(options=pipeline_options) as p:
        examples = (p | 'ReadExamples' >> tfrecordio.ReadFromTFRecord(
            file_pattern="/home/burn/Downloads/ee-docs-demos-berlin/Image_pixel_demo_*.tfrecord.gz",
            compression_type=CompressionTypes.GZIP)
                    | 'ParseTFrecord' >> beam.Map(parse_tfrecord)
                    | "print" >> beam.Map(print))

        # Do we need to filter anything here?

        result = p.run()

if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    run()
